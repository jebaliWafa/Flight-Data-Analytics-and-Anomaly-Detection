{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "df76c83a-2ef5-486c-8836-93ad5fbd8b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-eventhub in /local_disk0/.ephemeral_nfs/envs/pythonEnv-abdc13a1-3a5b-4baf-b8eb-167728a66e10/lib/python3.12/site-packages (5.13.0)\n",
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (2.32.2)\n",
      "Requirement already satisfied: azure-core>=1.27.0 in /databricks/python3/lib/python3.12/site-packages (from azure-eventhub) (1.32.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from azure-eventhub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.27.0->azure-eventhub) (1.16.0)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pip install azure-eventhub requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "22ba4361-a2dc-4a56-8563-7c0ef767212e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da64e267-62a0-494d-b66d-ee7ac6a91e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Batch Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf4b853a-9189-47e3-bba8-f519c17a2577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6807417c-0e72-4f19-9b6b-a122aa90e0c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading the csv file"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/naima.rejeb@polytechnicien.tn/states_2017_06_05_00.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7284fda7-c3cb-41d2-8dcd-6db7e83e754f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: string (nullable = true)\n",
      " |-- icao24: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- lon: string (nullable = true)\n",
      " |-- velocity: string (nullable = true)\n",
      " |-- heading: string (nullable = true)\n",
      " |-- vertrate: string (nullable = true)\n",
      " |-- callsign: string (nullable = true)\n",
      " |-- onground: string (nullable = true)\n",
      " |-- alert: string (nullable = true)\n",
      " |-- spi: string (nullable = true)\n",
      " |-- squawk: string (nullable = true)\n",
      " |-- baroaltitude: string (nullable = true)\n",
      " |-- geoaltitude: string (nullable = true)\n",
      " |-- lastposupdate: string (nullable = true)\n",
      " |-- lastcontact: string (nullable = true)\n",
      "\n",
      "+----------+------+-------------+--------------+-------------+-------------+---------+--------+--------+-----+-----+------+------------+-----------+-------------+------------+\n",
      "|      time|icao24|          lat|           lon|     velocity|      heading| vertrate|callsign|onground|alert|  spi|squawk|baroaltitude|geoaltitude|lastposupdate| lastcontact|\n",
      "+----------+------+-------------+--------------+-------------+-------------+---------+--------+--------+-----+-----+------+------------+-----------+-------------+------------+\n",
      "|1496620800|4bccb9| 52.084915638| 11.4453935623|175.320556641|305.321044922|     NULL|SXS2WY  |   False|False|False|  1000|     7459.98|     7581.9| 1496620800.0|1496620800.0|\n",
      "|1496620800|502cb2|49.6373748779| 2.87184062757| 232.93635757|305.199349779| -0.32512|MON55BR |   False|False|False|  2770|    10988.04|   11033.76|1496620797.81|1496620800.0|\n",
      "|1496620800|4bccaf|49.8504638672| 12.5227832794|196.684570312|288.764648438|     NULL|SXS7R   |   False|False|False|  3215|     11582.4|    11772.9| 1496620800.0|1496620800.0|\n",
      "|1496620800|4008e1|50.7808470726|   9.262611866|214.205322266|292.747192383|     NULL|TCX229  |   False|False|False|  3462|     10363.2|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|4070e3|50.7229924606| 3.82369995117|221.822621523|297.634765076|  0.32512|EXS14D  |   False|False|False|  7775|     10972.8|   11033.76|1496620797.84|1496620800.0|\n",
      "|1496620800|44d1cc|50.1474165916| 11.2171912193|242.690673828|94.8669433594|      0.0|TAY289S |   False|False|False|  7174|     10668.0|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|c04081|51.4994001389| 7.58133888245|222.683105469|310.127563477|  -293.37|TFL230  |   False|False|False|  1000|    11414.76|   11452.86| 1496620800.0|1496620800.0|\n",
      "|1496620800|405f09|49.6737670898|-1.57979061729|225.925273748|10.3631924562| -8.45312|EZY12VM |   False|False|False|  1056|     9669.78|    9685.02|1496620798.87|1496620800.0|\n",
      "|1496620800|34310d|51.8420577049| 12.5282979012|218.726806641| 37.353515625|   681.99|FAH5102 |   False|False|False|  3611|     5433.06|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|4bccab|49.9548768997| 12.0416164398|204.484130859|288.479003906|      0.0|SXS4XY  |   False|False|False|  3207|     11582.4|   11719.56| 1496620800.0|1496620800.0|\n",
      "|1496620800|400bdb|50.7147992668|-2.71018981934|175.305426615|13.7507864301| -7.80288|EZY53TU |   False|False|False|  7370|     4023.36|    4046.22|1496620798.95|1496620800.0|\n",
      "|1496620800|406696|50.7993846829| 4.70260620117|216.956960387|296.929600528|      0.0|TOM66A  |   False|False|False|  7771|    11574.78|   11650.98|1496620797.49|1496620800.0|\n",
      "|1496620800|400a30|51.5809502036| 4.43023681641|207.463216937|282.313063459|      0.0|TCX779  |   False|False|False|  7770|     10363.2|   10370.82|1496620797.21|1496620800.0|\n",
      "|1496620800|45aa84| 53.693253994| 14.3268156052|211.266357422|308.572998047|      0.0|JTG678  |   False|False|False|  3214|    10965.18|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|502cb5|55.1351666451| 9.52079057693|252.864013672|37.4743652344|      0.0|PRW724  |   False|False|False|  7577|    12504.42|   12512.04| 1496620800.0|1496620800.0|\n",
      "|1496620800|406953|51.3621081336|0.954971313477| 243.67980168|103.801546247|  0.32512|EDC533  |   False|False|False|  7274|      8839.2|    8907.78|1496620798.99|1496620800.0|\n",
      "|1496620800|34440c|51.3345492088|-1.81510925293|182.603707657|274.200622205|-11.05408|VLG7758 |   False|False|False|  7770|     5417.82|    5471.16| 1496620798.6|1496620800.0|\n",
      "|1496620800|780de5|53.7846851349| 10.8065128326|268.689208984|37.9248046875|      0.0|CES710  |   False|False|False|  2312|     10363.2|   10431.78| 1496620800.0|1496620800.0|\n",
      "|1496620800|89629b|49.8300361633| 10.5671310425|258.063720703|101.266479492|  371.475|ETD946  |   False|False|False|  7675|    10584.18|   10736.58| 1496620800.0|1496620800.0|\n",
      "|1496620800|4bcde4|52.1378087997| 11.2346148491|148.530761719|292.390136719|     NULL|SXS8VH  |   False|False|False|  3205|     5745.48|    5836.92| 1496620800.0|1496620800.0|\n",
      "+----------+------+-------------+--------------+-------------+-------------+---------+--------+--------+-----+-----+------+------------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fdceeb9-e56e-42d9-9120-36c9e300cb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Create temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "85cbac65-adaa-4c85-ac1d-fc4982bbdb72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"raw_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3db55e1-2303-4824-8c26-11b03efa427f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d6d76d-0330-4d76-97c2-4e27c95a5853",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------+--------------+-------------+-------------+--------+--------+--------+-----+-----+------+------------+-----------+-------------+------------+\n",
      "|      time|icao24|          lat|           lon|     velocity|      heading|vertrate|callsign|onground|alert|  spi|squawk|baroaltitude|geoaltitude|lastposupdate| lastcontact|\n",
      "+----------+------+-------------+--------------+-------------+-------------+--------+--------+--------+-----+-----+------+------------+-----------+-------------+------------+\n",
      "|1496620800|4bccb9| 52.084915638| 11.4453935623|175.320556641|305.321044922|    NULL|SXS2WY  |   False|False|False|  1000|     7459.98|     7581.9| 1496620800.0|1496620800.0|\n",
      "|1496620800|502cb2|49.6373748779| 2.87184062757| 232.93635757|305.199349779|-0.32512|MON55BR |   False|False|False|  2770|    10988.04|   11033.76|1496620797.81|1496620800.0|\n",
      "|1496620800|4bccaf|49.8504638672| 12.5227832794|196.684570312|288.764648438|    NULL|SXS7R   |   False|False|False|  3215|     11582.4|    11772.9| 1496620800.0|1496620800.0|\n",
      "|1496620800|4008e1|50.7808470726|   9.262611866|214.205322266|292.747192383|    NULL|TCX229  |   False|False|False|  3462|     10363.2|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|4070e3|50.7229924606| 3.82369995117|221.822621523|297.634765076| 0.32512|EXS14D  |   False|False|False|  7775|     10972.8|   11033.76|1496620797.84|1496620800.0|\n",
      "|1496620800|44d1cc|50.1474165916| 11.2171912193|242.690673828|94.8669433594|     0.0|TAY289S |   False|False|False|  7174|     10668.0|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|c04081|51.4994001389| 7.58133888245|222.683105469|310.127563477| -293.37|TFL230  |   False|False|False|  1000|    11414.76|   11452.86| 1496620800.0|1496620800.0|\n",
      "|1496620800|405f09|49.6737670898|-1.57979061729|225.925273748|10.3631924562|-8.45312|EZY12VM |   False|False|False|  1056|     9669.78|    9685.02|1496620798.87|1496620800.0|\n",
      "|1496620800|34310d|51.8420577049| 12.5282979012|218.726806641| 37.353515625|  681.99|FAH5102 |   False|False|False|  3611|     5433.06|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|4bccab|49.9548768997| 12.0416164398|204.484130859|288.479003906|     0.0|SXS4XY  |   False|False|False|  3207|     11582.4|   11719.56| 1496620800.0|1496620800.0|\n",
      "+----------+------+-------------+--------------+-------------+-------------+--------+--------+--------+-----+-----+------+------------+-----------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM raw_data LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c86b77-99c7-45f1-86df-9e5315dcac36",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1526689|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Inspecting data\n",
    "spark.sql(\"SELECT COUNT(*) FROM raw_data\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b5c057-173e-44ec-b465-ada9af744412",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|icao24|\n",
      "+------+\n",
      "|45aa84|\n",
      "|c04081|\n",
      "|502cb5|\n",
      "|4bcde4|\n",
      "|400bdb|\n",
      "|400a30|\n",
      "|34310d|\n",
      "|89629b|\n",
      "|502cb2|\n",
      "|44d1cc|\n",
      "|406953|\n",
      "|4bccb9|\n",
      "|34440c|\n",
      "|406696|\n",
      "|780de5|\n",
      "|4bccaf|\n",
      "|4bccab|\n",
      "|4008e1|\n",
      "|4070e3|\n",
      "|471f8c|\n",
      "+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT icao24 FROM raw_data\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c080c0-f03e-485c-b760-b953f8e7524d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+------------------+\n",
      "|      avg_velocity|         avg_lat|           avg_lon|\n",
      "+------------------+----------------+------------------+\n",
      "|183.06975094904107|31.9657066062951|-48.93514326517676|\n",
      "+------------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT AVG(velocity) AS avg_velocity, AVG(lat) AS avg_lat, AVG(lon) AS avg_lon\n",
    "    FROM raw_data\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39d366ab-d1f6-4c84-a609-8b63f5222fba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------+---------------+-------------+-------------+---------+--------+--------+-----+-----+------+------------+-----------+-------------+------------+\n",
      "|      time|icao24|          lat|            lon|     velocity|      heading| vertrate|callsign|onground|alert|  spi|squawk|baroaltitude|geoaltitude|lastposupdate| lastcontact|\n",
      "+----------+------+-------------+---------------+-------------+-------------+---------+--------+--------+-----+-----+------+------------+-----------+-------------+------------+\n",
      "|1496620800|4bccb9| 52.084915638|  11.4453935623|175.320556641|305.321044922|     NULL|SXS2WY  |   False|False|False|  1000|     7459.98|     7581.9| 1496620800.0|1496620800.0|\n",
      "|1496620800|4008e1|50.7808470726|    9.262611866|214.205322266|292.747192383|     NULL|TCX229  |   False|False|False|  3462|     10363.2|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|4070e3|50.7229924606|  3.82369995117|221.822621523|297.634765076|  0.32512|EXS14D  |   False|False|False|  7775|     10972.8|   11033.76|1496620797.84|1496620800.0|\n",
      "|1496620800|44d1cc|50.1474165916|  11.2171912193|242.690673828|94.8669433594|      0.0|TAY289S |   False|False|False|  7174|     10668.0|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|c04081|51.4994001389|  7.58133888245|222.683105469|310.127563477|  -293.37|TFL230  |   False|False|False|  1000|    11414.76|   11452.86| 1496620800.0|1496620800.0|\n",
      "|1496620800|34310d|51.8420577049|  12.5282979012|218.726806641| 37.353515625|   681.99|FAH5102 |   False|False|False|  3611|     5433.06|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|400bdb|50.7147992668| -2.71018981934|175.305426615|13.7507864301| -7.80288|EZY53TU |   False|False|False|  7370|     4023.36|    4046.22|1496620798.95|1496620800.0|\n",
      "|1496620800|406696|50.7993846829|  4.70260620117|216.956960387|296.929600528|      0.0|TOM66A  |   False|False|False|  7771|    11574.78|   11650.98|1496620797.49|1496620800.0|\n",
      "|1496620800|400a30|51.5809502036|  4.43023681641|207.463216937|282.313063459|      0.0|TCX779  |   False|False|False|  7770|     10363.2|   10370.82|1496620797.21|1496620800.0|\n",
      "|1496620800|45aa84| 53.693253994|  14.3268156052|211.266357422|308.572998047|      0.0|JTG678  |   False|False|False|  3214|    10965.18|       NULL| 1496620800.0|1496620800.0|\n",
      "|1496620800|502cb5|55.1351666451|  9.52079057693|252.864013672|37.4743652344|      0.0|PRW724  |   False|False|False|  7577|    12504.42|   12512.04| 1496620800.0|1496620800.0|\n",
      "|1496620800|406953|51.3621081336| 0.954971313477| 243.67980168|103.801546247|  0.32512|EDC533  |   False|False|False|  7274|      8839.2|    8907.78|1496620798.99|1496620800.0|\n",
      "|1496620800|34440c|51.3345492088| -1.81510925293|182.603707657|274.200622205|-11.05408|VLG7758 |   False|False|False|  7770|     5417.82|    5471.16| 1496620798.6|1496620800.0|\n",
      "|1496620800|780de5|53.7846851349|  10.8065128326|268.689208984|37.9248046875|      0.0|CES710  |   False|False|False|  2312|     10363.2|   10431.78| 1496620800.0|1496620800.0|\n",
      "|1496620800|4bcde4|52.1378087997|  11.2346148491|148.530761719|292.390136719|     NULL|SXS8VH  |   False|False|False|  3205|     5745.48|    5836.92| 1496620800.0|1496620800.0|\n",
      "|1496620800|471f8c|50.5348563194|  12.6516151428|245.629638672|109.193115234|      0.0|WZZ1300 |   False|False|False|  2213|    11269.98|   11422.38| 1496620800.0|1496620800.0|\n",
      "|1496620800|484f2e|50.3293991089|  13.1439614296|222.570068359|297.691040039|   -19.05|CND718  |   False|False|False|  1000|     12192.0|    12382.5| 1496620800.0|1496620800.0|\n",
      "|1496620800|a21679|51.6145849228|  13.1438112259|241.334228516|323.668212891|     NULL|UAL91   |   False|False|False|  7675|      9144.0|    9319.26| 1496620800.0|1496620800.0|\n",
      "|1496620800|c033a0|51.6022253036|  8.52189302444|216.126953125|297.663574219|   -19.05|TFL1YL  |   False|False|False|  1000|     11582.4|    11620.5| 1496620800.0|1496620800.0|\n",
      "|1496620800|400e4e|53.4152014781|-0.058889950023|185.671527499|287.079385813| -9.10336|EXS406  |   False|False|False|  3412|     5745.48|    5737.86|1496620798.62|1496620800.0|\n",
      "+----------+------+-------------+---------------+-------------+-------------+---------+--------+--------+-----+-----+------+------------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM raw_data\n",
    "    WHERE lat BETWEEN 50 AND 55\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e336e327-0a22-4f2b-b238-7bb2f3c0a513",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark SQL"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------+-----------+\n",
      "|max_velocity|min_velocity|  max_heading|min_heading|\n",
      "+------------+------------+-------------+-----------+\n",
      "|99.994204739|         0.0|99.9993781232|       -0.0|\n",
      "+------------+------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT MAX(velocity) AS max_velocity, MIN(velocity) AS min_velocity, \n",
    "           MAX(heading) AS max_heading, MIN(heading) AS min_heading\n",
    "    FROM raw_data\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a741f0da-8d53-4cd6-9acb-2eb7679f34d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dropping nan values"
    }
   },
   "outputs": [],
   "source": [
    "data_cleaned = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503fb0d4-944f-4633-a301-95b898c7d115",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Casting columns"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: integer (nullable = true)\n",
      " |-- icao24: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- velocity: double (nullable = true)\n",
      " |-- heading: double (nullable = true)\n",
      " |-- vertrate: double (nullable = true)\n",
      " |-- callsign: string (nullable = true)\n",
      " |-- onground: boolean (nullable = true)\n",
      " |-- alert: boolean (nullable = true)\n",
      " |-- spi: boolean (nullable = true)\n",
      " |-- squawk: integer (nullable = true)\n",
      " |-- baroaltitude: double (nullable = true)\n",
      " |-- geoaltitude: double (nullable = true)\n",
      " |-- lastposupdate: double (nullable = true)\n",
      " |-- lastcontact: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cast columns to appropriate data types\n",
    "data_cleaned = df.select(\n",
    "    col(\"time\").cast(\"int\"),\n",
    "    col(\"icao24\").cast(\"string\"),\n",
    "    col(\"lat\").cast(\"double\"),\n",
    "    col(\"lon\").cast(\"double\"),\n",
    "    col(\"velocity\").cast(\"double\"),\n",
    "    col(\"heading\").cast(\"double\"),\n",
    "    col(\"vertrate\").cast(\"double\"),\n",
    "    col(\"callsign\").cast(\"string\"),\n",
    "    col(\"onground\").cast(\"boolean\"),\n",
    "    col(\"alert\").cast(\"boolean\"),\n",
    "    col(\"spi\").cast(\"boolean\"),\n",
    "    col(\"squawk\").cast(\"int\"),\n",
    "    col(\"baroaltitude\").cast(\"double\"),\n",
    "    col(\"geoaltitude\").cast(\"double\"),\n",
    "    col(\"lastposupdate\").cast(\"double\"),\n",
    "    col(\"lastcontact\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Now, data_cleaned should have the correct data types\n",
    "data_cleaned.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb890cf0-ede1-4dbb-9691-677125d46f13",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cosmos DB Cassandra  Connection and Writing"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.cassandra.connection.host\", \"cassandraaccount.cassandra.cosmos.azure.com\")\n",
    "spark.conf.set(\"spark.cassandra.connection.port\", \"10350\")\n",
    "\n",
    "data_cleaned.limit(10).write.format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode('append') \\\n",
    "    .option(\"table\", \"flightdata\") \\\n",
    "    .option(\"keyspace\", \"flightdb\") \\\n",
    "    .option(\"spark.cassandra.connection.ssl.enabled\", \"true\") \\\n",
    "    .option(\"spark.cassandra.auth.username\", \"cassandraaccount\") \\\n",
    "    .option(\"spark.cassandra.auth.password\", \"##\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be1dd0e-f5f3-4f07-b589-4a8e370f5e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:717)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:458)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:712)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:67)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:690)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:874)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$7(Chauffeur.scala:900)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:899)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:954)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:742)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1032)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:952)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:521)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:766)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:766)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:729)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:711)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$16(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:521)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:411)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:126)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:123)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:717)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:458)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:712)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:67)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:690)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:874)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$7(Chauffeur.scala:900)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:899)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:954)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:742)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1032)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:952)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:521)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:766)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:766)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:729)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:711)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$16(ActivityContextFactory.scala:283)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:521)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:411)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:126)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:123)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.cassandra.connection.host\", \"cassandraaccount.cassandra.cosmos.azure.com\")\n",
    "spark.conf.set(\"spark.cassandra.connection.port\", \"10350\")\n",
    "\n",
    "data_cleaned.write.format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode('append') \\\n",
    "    .option(\"table\", \"flightdata\") \\\n",
    "    .option(\"keyspace\", \"flightdb\") \\\n",
    "    .option(\"spark.cassandra.connection.ssl.enabled\", \"true\") \\\n",
    "    .option(\"spark.cassandra.auth.username\", \"cassandraaccount\") \\\n",
    "    .option(\"spark.cassandra.auth.password\", \"##\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b7c7c3-4e02-4841-b780-4e1e8b2a911c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-7981590205821744>, line 8\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.cassandra.connection.host\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcassandraaccount.cassandra.cosmos.azure.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.cassandra.connection.port\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10350\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m      5\u001b[0m keyspaces_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread \\\n",
       "\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.cassandra\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n",
       "\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39moptions(table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_schema.keyspaces\u001b[39m\u001b[38;5;124m\"\u001b[39m, keyspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n",
       "\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     50\u001b[0m     )\n",
       "\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:319\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n",
       "\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n",
       "\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n",
       "\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:263\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n",
       "\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
       "\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    265\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o969.load.\n",
       ": java.io.IOException: Failed to open native connection to Cassandra at {cassandraaccount.cassandra.cosmos.azure.com:10350} :: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=581ce20a): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:173)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$sessionCache$1(CassandraConnector.scala:161)\n",
       "\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n",
       "\tat com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n",
       "\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)\n",
       "\tat com.datastax.spark.connector.datasource.CassandraCatalog$.com$datastax$spark$connector$datasource$CassandraCatalog$$getMetadata(CassandraCatalog.scala:476)\n",
       "\tat com.datastax.spark.connector.datasource.CassandraCatalog$.getRelationMetaData(CassandraCatalog.scala:432)\n",
       "\tat org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:63)\n",
       "\tat org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:67)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:103)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:244)\n",
       "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:345)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:343)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:233)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: com.datastax.oss.driver.api.core.AllNodesFailedException: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=581ce20a): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n",
       "\tat com.datastax.oss.driver.api.core.AllNodesFailedException.copy(AllNodesFailedException.java:141)\n",
       "\tat com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149)\n",
       "\tat com.datastax.oss.driver.api.core.session.SessionBuilder.build(SessionBuilder.java:835)\n",
       "\tat com.datastax.spark.connector.cql.DefaultConnectionFactory$.createSession(CassandraConnectionFactory.scala:143)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:167)\n",
       "\t... 28 more\n",
       "\tSuppressed: com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler$InitRequest.fail(ProtocolInitHandler.java:356)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.ChannelHandlerRequest.onFailure(ChannelHandlerRequest.java:104)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.fail(InFlightHandler.java:381)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:371)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:353)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.channelInactive(InFlightHandler.java:331)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
       "\t\t... 1 more\n",
       "\tCaused by: com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o969.load.\n: java.io.IOException: Failed to open native connection to Cassandra at {cassandraaccount.cassandra.cosmos.azure.com:10350} :: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=581ce20a): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:173)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$sessionCache$1(CassandraConnector.scala:161)\n\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n\tat com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.com$datastax$spark$connector$datasource$CassandraCatalog$$getMetadata(CassandraCatalog.scala:476)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.getRelationMetaData(CassandraCatalog.scala:432)\n\tat org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:63)\n\tat org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:67)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:103)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:244)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:345)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:343)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:233)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.datastax.oss.driver.api.core.AllNodesFailedException: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=581ce20a): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.oss.driver.api.core.AllNodesFailedException.copy(AllNodesFailedException.java:141)\n\tat com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149)\n\tat com.datastax.oss.driver.api.core.session.SessionBuilder.build(SessionBuilder.java:835)\n\tat com.datastax.spark.connector.cql.DefaultConnectionFactory$.createSession(CassandraConnectionFactory.scala:143)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:167)\n\t... 28 more\n\tSuppressed: com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)\n\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler$InitRequest.fail(ProtocolInitHandler.java:356)\n\t\tat com.datastax.oss.driver.internal.core.channel.ChannelHandlerRequest.onFailure(ChannelHandlerRequest.java:104)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.fail(InFlightHandler.java:381)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:371)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:353)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.channelInactive(InFlightHandler.java:331)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\t\tat com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\n\tCaused by: com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o969.load.\n: java.io.IOException: Failed to open native connection to Cassandra at {cassandraaccount.cassandra.cosmos.azure.com:10350} :: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=581ce20a): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:173)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$sessionCache$1(CassandraConnector.scala:161)\n\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n\tat com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.com$datastax$spark$connector$datasource$CassandraCatalog$$getMetadata(CassandraCatalog.scala:476)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.getRelationMetaData(CassandraCatalog.scala:432)\n\tat org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:63)\n\tat org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:67)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:103)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:244)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:345)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:343)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:233)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.datastax.oss.driver.api.core.AllNodesFailedException: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=581ce20a): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.oss.driver.api.core.AllNodesFailedException.copy(AllNodesFailedException.java:141)\n\tat com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149)\n\tat com.datastax.oss.driver.api.core.session.SessionBuilder.build(SessionBuilder.java:835)\n\tat com.datastax.spark.connector.cql.DefaultConnectionFactory$.createSession(CassandraConnectionFactory.scala:143)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:167)\n\t... 28 more\n\tSuppressed: com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)\n\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler$InitRequest.fail(ProtocolInitHandler.java:356)\n\t\tat com.datastax.oss.driver.internal.core.channel.ChannelHandlerRequest.onFailure(ChannelHandlerRequest.java:104)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.fail(InFlightHandler.java:381)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:371)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:353)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.channelInactive(InFlightHandler.java:331)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\t\tat com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\n\tCaused by: com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "File \u001b[0;32m<command-7981590205821744>, line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.cassandra.connection.host\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcassandraaccount.cassandra.cosmos.azure.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.cassandra.connection.port\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10350\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m keyspaces_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.cassandra\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39moptions(table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_schema.keyspaces\u001b[39m\u001b[38;5;124m\"\u001b[39m, keyspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:319\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n",
        "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:263\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    265\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
        "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o969.load.\n: java.io.IOException: Failed to open native connection to Cassandra at {cassandraaccount.cassandra.cosmos.azure.com:10350} :: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=581ce20a): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:173)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$sessionCache$1(CassandraConnector.scala:161)\n\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n\tat com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.com$datastax$spark$connector$datasource$CassandraCatalog$$getMetadata(CassandraCatalog.scala:476)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.getRelationMetaData(CassandraCatalog.scala:432)\n\tat org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:63)\n\tat org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:67)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:103)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:244)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:345)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:343)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:233)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.datastax.oss.driver.api.core.AllNodesFailedException: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=581ce20a): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.oss.driver.api.core.AllNodesFailedException.copy(AllNodesFailedException.java:141)\n\tat com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149)\n\tat com.datastax.oss.driver.api.core.session.SessionBuilder.build(SessionBuilder.java:835)\n\tat com.datastax.spark.connector.cql.DefaultConnectionFactory$.createSession(CassandraConnectionFactory.scala:143)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:167)\n\t... 28 more\n\tSuppressed: com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s14|control|id: 0x752e71b3, L:/10.139.64.4:42114 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)\n\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler$InitRequest.fail(ProtocolInitHandler.java:356)\n\t\tat com.datastax.oss.driver.internal.core.channel.ChannelHandlerRequest.onFailure(ChannelHandlerRequest.java:104)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.fail(InFlightHandler.java:381)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:371)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:353)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.channelInactive(InFlightHandler.java:331)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\t\tat com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\n\tCaused by: com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.cassandra.connection.host\", \"cassandraaccount.cassandra.cosmos.azure.com\")\n",
    "spark.conf.set(\"spark.cassandra.connection.port\", \"10350\")\n",
    "\n",
    "\n",
    "keyspaces_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"system_schema.keyspaces\", keyspace=\"system_schema\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6a11404-6f25-45c5-a743-0086a6e1ea02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-7981590205821745>, line 3\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.cassandra\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n",
       "\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39moptions(table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflight_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, keyspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflightdb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n",
       "\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     50\u001b[0m     )\n",
       "\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:319\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n",
       "\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n",
       "\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n",
       "\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:263\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n",
       "\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
       "\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    265\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o946.load.\n",
       ": java.io.IOException: Failed to open native connection to Cassandra at {cassandraaccount.cassandra.cosmos.azure.com:10350} :: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=1b4eece6): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:173)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$sessionCache$1(CassandraConnector.scala:161)\n",
       "\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n",
       "\tat com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n",
       "\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)\n",
       "\tat com.datastax.spark.connector.datasource.CassandraCatalog$.com$datastax$spark$connector$datasource$CassandraCatalog$$getMetadata(CassandraCatalog.scala:476)\n",
       "\tat com.datastax.spark.connector.datasource.CassandraCatalog$.getRelationMetaData(CassandraCatalog.scala:432)\n",
       "\tat org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:63)\n",
       "\tat org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:67)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:103)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:244)\n",
       "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:345)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:343)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:233)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: com.datastax.oss.driver.api.core.AllNodesFailedException: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=1b4eece6): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n",
       "\tat com.datastax.oss.driver.api.core.AllNodesFailedException.copy(AllNodesFailedException.java:141)\n",
       "\tat com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149)\n",
       "\tat com.datastax.oss.driver.api.core.session.SessionBuilder.build(SessionBuilder.java:835)\n",
       "\tat com.datastax.spark.connector.cql.DefaultConnectionFactory$.createSession(CassandraConnectionFactory.scala:143)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:167)\n",
       "\t... 28 more\n",
       "\tSuppressed: com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler$InitRequest.fail(ProtocolInitHandler.java:356)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.ChannelHandlerRequest.onFailure(ChannelHandlerRequest.java:104)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.fail(InFlightHandler.java:381)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:371)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:353)\n",
       "\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.channelInactive(InFlightHandler.java:331)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
       "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
       "\t\t... 1 more\n",
       "\tCaused by: com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o946.load.\n: java.io.IOException: Failed to open native connection to Cassandra at {cassandraaccount.cassandra.cosmos.azure.com:10350} :: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=1b4eece6): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:173)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$sessionCache$1(CassandraConnector.scala:161)\n\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n\tat com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.com$datastax$spark$connector$datasource$CassandraCatalog$$getMetadata(CassandraCatalog.scala:476)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.getRelationMetaData(CassandraCatalog.scala:432)\n\tat org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:63)\n\tat org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:67)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:103)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:244)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:345)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:343)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:233)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.datastax.oss.driver.api.core.AllNodesFailedException: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=1b4eece6): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.oss.driver.api.core.AllNodesFailedException.copy(AllNodesFailedException.java:141)\n\tat com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149)\n\tat com.datastax.oss.driver.api.core.session.SessionBuilder.build(SessionBuilder.java:835)\n\tat com.datastax.spark.connector.cql.DefaultConnectionFactory$.createSession(CassandraConnectionFactory.scala:143)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:167)\n\t... 28 more\n\tSuppressed: com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)\n\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler$InitRequest.fail(ProtocolInitHandler.java:356)\n\t\tat com.datastax.oss.driver.internal.core.channel.ChannelHandlerRequest.onFailure(ChannelHandlerRequest.java:104)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.fail(InFlightHandler.java:381)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:371)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:353)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.channelInactive(InFlightHandler.java:331)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\t\tat com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\n\tCaused by: com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o946.load.\n: java.io.IOException: Failed to open native connection to Cassandra at {cassandraaccount.cassandra.cosmos.azure.com:10350} :: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=1b4eece6): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:173)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$sessionCache$1(CassandraConnector.scala:161)\n\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n\tat com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.com$datastax$spark$connector$datasource$CassandraCatalog$$getMetadata(CassandraCatalog.scala:476)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.getRelationMetaData(CassandraCatalog.scala:432)\n\tat org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:63)\n\tat org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:67)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:103)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:244)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:345)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:343)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:233)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.datastax.oss.driver.api.core.AllNodesFailedException: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=1b4eece6): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.oss.driver.api.core.AllNodesFailedException.copy(AllNodesFailedException.java:141)\n\tat com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149)\n\tat com.datastax.oss.driver.api.core.session.SessionBuilder.build(SessionBuilder.java:835)\n\tat com.datastax.spark.connector.cql.DefaultConnectionFactory$.createSession(CassandraConnectionFactory.scala:143)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:167)\n\t... 28 more\n\tSuppressed: com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)\n\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler$InitRequest.fail(ProtocolInitHandler.java:356)\n\t\tat com.datastax.oss.driver.internal.core.channel.ChannelHandlerRequest.onFailure(ChannelHandlerRequest.java:104)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.fail(InFlightHandler.java:381)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:371)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:353)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.channelInactive(InFlightHandler.java:331)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\t\tat com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\n\tCaused by: com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "File \u001b[0;32m<command-7981590205821745>, line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.cassandra\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39moptions(table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflight_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, keyspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflightdb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:319\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n",
        "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:263\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    265\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
        "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o946.load.\n: java.io.IOException: Failed to open native connection to Cassandra at {cassandraaccount.cassandra.cosmos.azure.com:10350} :: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=1b4eece6): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:173)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$sessionCache$1(CassandraConnector.scala:161)\n\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n\tat com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.com$datastax$spark$connector$datasource$CassandraCatalog$$getMetadata(CassandraCatalog.scala:476)\n\tat com.datastax.spark.connector.datasource.CassandraCatalog$.getRelationMetaData(CassandraCatalog.scala:432)\n\tat org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:63)\n\tat org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:67)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:103)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:244)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:345)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:343)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:233)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.datastax.oss.driver.api.core.AllNodesFailedException: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350, hostId=null, hashCode=1b4eece6): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)]\n\tat com.datastax.oss.driver.api.core.AllNodesFailedException.copy(AllNodesFailedException.java:141)\n\tat com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149)\n\tat com.datastax.oss.driver.api.core.session.SessionBuilder.build(SessionBuilder.java:835)\n\tat com.datastax.spark.connector.cql.DefaultConnectionFactory$.createSession(CassandraConnectionFactory.scala:143)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:167)\n\t... 28 more\n\tSuppressed: com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s11|control|id: 0xcdfd4bcd, L:/10.139.64.4:55334 - R:cassandraaccount.cassandra.cosmos.azure.com/20.166.46.168:10350] Protocol initialization request, step 1 (OPTIONS): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer)\n\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler$InitRequest.fail(ProtocolInitHandler.java:356)\n\t\tat com.datastax.oss.driver.internal.core.channel.ChannelHandlerRequest.onFailure(ChannelHandlerRequest.java:104)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.fail(InFlightHandler.java:381)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:371)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.abortAllInFlight(InFlightHandler.java:353)\n\t\tat com.datastax.oss.driver.internal.core.channel.InFlightHandler.channelInactive(InFlightHandler.java:331)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)\n\t\tat com.datastax.oss.driver.shaded.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\t\tat com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\n\tCaused by: com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Lost connection to remote peer\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"flight_data\", keyspace=\"flightdb\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d4b3a8-64dc-4721-8d02-a7b28104be76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Graph creation"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Data:\n",
      "+----------+------+-------------+--------------+-------------+-------------+--------+--------+--------+-----+-----+------+------------+-----------+---------------+-----------+\n",
      "|      time|icao24|          lat|           lon|     velocity|      heading|vertrate|callsign|onground|alert|  spi|squawk|baroaltitude|geoaltitude|  lastposupdate|lastcontact|\n",
      "+----------+------+-------------+--------------+-------------+-------------+--------+--------+--------+-----+-----+------+------------+-----------+---------------+-----------+\n",
      "|1496620800|502cb2|49.6373748779| 2.87184062757| 232.93635757|305.199349779|-0.32512|MON55BR |   false|false|false|  2770|    10988.04|   11033.76|1.49662079781E9|1.4966208E9|\n",
      "|1496620800|4070e3|50.7229924606| 3.82369995117|221.822621523|297.634765076| 0.32512|EXS14D  |   false|false|false|  7775|     10972.8|   11033.76|1.49662079784E9|1.4966208E9|\n",
      "|1496620800|c04081|51.4994001389| 7.58133888245|222.683105469|310.127563477| -293.37|TFL230  |   false|false|false|  1000|    11414.76|   11452.86|    1.4966208E9|1.4966208E9|\n",
      "|1496620800|405f09|49.6737670898|-1.57979061729|225.925273748|10.3631924562|-8.45312|EZY12VM |   false|false|false|  1056|     9669.78|    9685.02|1.49662079887E9|1.4966208E9|\n",
      "|1496620800|4bccab|49.9548768997| 12.0416164398|204.484130859|288.479003906|     0.0|SXS4XY  |   false|false|false|  3207|     11582.4|   11719.56|    1.4966208E9|1.4966208E9|\n",
      "|1496620800|400bdb|50.7147992668|-2.71018981934|175.305426615|13.7507864301|-7.80288|EZY53TU |   false|false|false|  7370|     4023.36|    4046.22|1.49662079895E9|1.4966208E9|\n",
      "|1496620800|406696|50.7993846829| 4.70260620117|216.956960387|296.929600528|     0.0|TOM66A  |   false|false|false|  7771|    11574.78|   11650.98|1.49662079749E9|1.4966208E9|\n",
      "|1496620800|400a30|51.5809502036| 4.43023681641|207.463216937|282.313063459|     0.0|TCX779  |   false|false|false|  7770|     10363.2|   10370.82|1.49662079721E9|1.4966208E9|\n",
      "|1496620800|502cb5|55.1351666451| 9.52079057693|252.864013672|37.4743652344|     0.0|PRW724  |   false|false|false|  7577|    12504.42|   12512.04|    1.4966208E9|1.4966208E9|\n",
      "|1496620800|406953|51.3621081336|0.954971313477| 243.67980168|103.801546247| 0.32512|EDC533  |   false|false|false|  7274|      8839.2|    8907.78|1.49662079899E9|1.4966208E9|\n",
      "+----------+------+-------------+--------------+-------------+-------------+--------+--------+--------+-----+-----+------+------------+-----------+---------------+-----------+\n",
      "only showing top 10 rows\n",
      "Vertices:\n",
      "+------+\n",
      "|    id|\n",
      "+------+\n",
      "|400ff4|\n",
      "|c04081|\n",
      "|4054a8|\n",
      "|502cb5|\n",
      "|400bdb|\n",
      "|400a30|\n",
      "|aa6f17|\n",
      "|89629b|\n",
      "|502cb2|\n",
      "|406953|\n",
      "|8681bd|\n",
      "|34440c|\n",
      "|406696|\n",
      "|780de5|\n",
      "|4bccab|\n",
      "|4070e3|\n",
      "|471f8c|\n",
      "|c033a0|\n",
      "|484f2e|\n",
      "|405f09|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "Edges:\n",
      "+------+------+---------+\n",
      "|   src|   dst|time_diff|\n",
      "+------+------+---------+\n",
      "|502cb2|4070e3|        0|\n",
      "|502cb2|c04081|        0|\n",
      "|502cb2|405f09|        0|\n",
      "|502cb2|4bccab|        0|\n",
      "|502cb2|400bdb|        0|\n",
      "|502cb2|406696|        0|\n",
      "|502cb2|400a30|        0|\n",
      "|502cb2|502cb5|        0|\n",
      "|502cb2|406953|        0|\n",
      "|502cb2|34440c|        0|\n",
      "|502cb2|780de5|        0|\n",
      "|502cb2|89629b|        0|\n",
      "|502cb2|471f8c|        0|\n",
      "|502cb2|484f2e|        0|\n",
      "|502cb2|c033a0|        0|\n",
      "|502cb2|400e4e|        0|\n",
      "|502cb2|400ff4|        0|\n",
      "|502cb2|4054a8|        0|\n",
      "|502cb2|aa6f17|      -10|\n",
      "|502cb2|8681bd|      -10|\n",
      "+------+------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the time range for the subset (replace with actual Unix timestamp values)\n",
    "start_time = 1496620800  # Example: June 5, 2017, 00:00:00 (Unix timestamp)\n",
    "end_time = start_time + 3 * 3600  # 3 hours later\n",
    "\n",
    "# Filter the data to create a subset\n",
    "subset_data = data_cleaned.filter(col(\"time\").between(start_time, end_time))\n",
    "\n",
    "# Display the filtered subset to verify data\n",
    "print(\"Subset Data:\")\n",
    "subset_data.show(10)\n",
    "\n",
    "# Prepare Vertices and Edges from the subset\n",
    "vertices = subset_data.select(\"icao24\").distinct().withColumnRenamed(\"icao24\", \"id\")\n",
    "\n",
    "edges = subset_data.alias(\"df1\") \\\n",
    "    .join(subset_data.alias(\"df2\"), \n",
    "          (col(\"df1.time\") - col(\"df2.time\")).between(-300, 300) & \n",
    "          (col(\"df1.icao24\") != col(\"df2.icao24\")), \"inner\") \\\n",
    "    .select(\n",
    "        col(\"df1.icao24\").alias(\"src\"),\n",
    "        col(\"df2.icao24\").alias(\"dst\"),\n",
    "        (col(\"df1.time\") - col(\"df2.time\")).alias(\"time_diff\")\n",
    "    )\n",
    "\n",
    "# Create GraphFrame\n",
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "# Display the graph structure\n",
    "print(\"Vertices:\")\n",
    "graph.vertices.show()\n",
    "\n",
    "print(\"Edges:\")\n",
    "graph.edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b89d4e4-7af6-49bd-bcb9-18b0cf4e7b94",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Graph Visualization"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:717)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:458)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:712)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:67)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:690)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:874)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$7(Chauffeur.scala:900)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:899)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:954)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:742)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1032)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:952)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:521)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:766)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:766)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:729)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:711)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$16(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:521)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:411)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:126)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:123)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:717)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:458)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:712)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:67)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:690)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:874)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$7(Chauffeur.scala:900)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:899)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:954)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:742)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1032)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:952)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:521)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:766)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:766)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:729)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:711)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$16(ActivityContextFactory.scala:283)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:521)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:411)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:126)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:123)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Limit the data size to avoid memory issues\n",
    "vertices_pd = graph.vertices.limit(100).toPandas()\n",
    "edges_pd = graph.edges.limit(200).toPandas()\n",
    "\n",
    "# Create NetworkX graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges\n",
    "G.add_nodes_from(vertices_pd['id'])\n",
    "G.add_edges_from(zip(edges_pd['src'], edges_pd['dst']))\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(G, with_labels=True, node_size=700, node_color=\"lightblue\", font_size=10)\n",
    "plt.title(\"Flight Graph Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c94a789d-d117-4b3d-bff0-32df5ce6ab82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Stream Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97edd816-9ae6-429a-8e53-1f83d7d182aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb85f4b69164a4b87e703443c00caa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe954f7172b4e43ae5c8154397e8aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+--------+-------------+------------+---------------------------------------------------------------------------------------------------------------+------------------------------------------+----------+-------------------------------------------+---------------------+\n",
      "|longitude|latitude|baro_altitude|velocity|vertical_rate|geo_altitude|features                                                                                                       |probability                               |prediction|probability_array                          |anomaly_score        |\n",
      "+---------+--------+-------------+--------+-------------+------------+---------------------------------------------------------------------------------------------------------------+------------------------------------------+----------+-------------------------------------------+---------------------+\n",
      "|6.4201   |50.0547 |9144.0       |201.7   |0.0          |8976.36     |[6.420100212097168,50.0546989440918,9144.0,201.6999969482422,0.0,8976.3603515625]                              |[0.9611846494242329,0.0388153505757671]   |0         |[0.9611846494242329, 0.0388153505757671]   |0.0388153505757671   |\n",
      "|-1.6501  |42.7587 |11887.2      |210.47  |0.0          |11932.92    |[-1.6500999927520752,42.75870132446289,11887.2001953125,210.47000122070312,0.0,11932.919921875]                |[0.9999462901048084,5.370989519148367E-5] |0         |[0.9999462901048084, 5.370989519148367E-5] |5.370989519148367E-5 |\n",
      "|3.0584   |41.8713 |11887.2      |209.94  |0.0          |11940.54    |[3.0583999156951904,41.871299743652344,11887.2001953125,209.94000244140625,0.0,11940.5400390625]               |[0.9998417997971542,1.5820020284585933E-4]|0         |[0.9998417997971542, 1.5820020284585933E-4]|1.5820020284585933E-4|\n",
      "|-0.8806  |47.0685 |11277.6      |205.54  |0.0          |11186.16    |[-0.8805999755859375,47.06850051879883,11277.599609375,205.5399932861328,0.0,11186.16015625]                   |[0.9997442519719723,2.5574802802767586E-4]|0         |[0.9997442519719723, 2.5574802802767586E-4]|2.5574802802767586E-4|\n",
      "|0.5914   |41.1241 |11887.2      |206.62  |0.0          |11971.02    |[0.5914000272750854,41.12409973144531,11887.2001953125,206.6199951171875,0.0,11971.01953125]                   |[0.9999422260440465,5.777395595342071E-5] |0         |[0.9999422260440465, 5.777395595342071E-5] |5.777395595342071E-5 |\n",
      "|-1.5838  |50.4458 |11277.6      |211.71  |0.0          |11026.14    |[-1.583799958229065,50.44580078125,11277.599609375,211.7100067138672,0.0,11026.1396484375]                     |[0.9999247923244042,7.520767559573604E-5] |0         |[0.9999247923244042, 7.520767559573604E-5] |7.520767559573604E-5 |\n",
      "|-1.3714  |47.4987 |11262.36     |203.45  |1.3          |11140.44    |[-1.371399998664856,47.49869918823242,11262.3603515625,203.4499969482422,1.2999999523162842,11140.4404296875]  |[0.9998294900097445,1.7050999025547913E-4]|0         |[0.9998294900097445, 1.7050999025547913E-4]|1.7050999025547913E-4|\n",
      "|4.2831   |43.2077 |11894.82     |243.25  |0.0          |11910.06    |[4.283100128173828,43.207698822021484,11894.8203125,243.25,0.0,11910.0595703125]                               |[0.9990055884126146,9.944115873855225E-4] |0         |[0.9990055884126146, 9.944115873855225E-4] |9.944115873855225E-4 |\n",
      "|-2.2109  |45.2683 |11887.2      |213.71  |0.0          |11864.34    |[-2.210900068283081,45.2682991027832,11887.2001953125,213.7100067138672,0.0,11864.33984375]                    |[0.9998454880136279,1.5451198637199043E-4]|0         |[0.9998454880136279, 1.5451198637199043E-4]|1.5451198637199043E-4|\n",
      "|-4.3349  |41.4403 |11285.22     |221.91  |0.0          |11346.18    |[-4.33489990234375,41.44029998779297,11285.2197265625,221.91000366210938,0.0,11346.1796875]                    |[0.9998992475485706,1.0075245142940658E-4]|0         |[0.9998992475485706, 1.0075245142940658E-4]|1.0075245142940658E-4|\n",
      "|8.8238   |48.7271 |10957.56     |211.97  |0.0          |10820.4     |[8.823800086975098,48.72710037231445,10957.5595703125,211.97000122070312,0.0,10820.400390625]                  |[0.9988331976894219,0.0011668023105781146]|0         |[0.9988331976894219, 0.0011668023105781146]|0.0011668023105781146|\n",
      "|5.9009   |50.7898 |11574.78     |201.78  |-0.65        |11369.04    |[5.900899887084961,50.789798736572266,11574.7802734375,201.77999877929688,-0.6499999761581421,11369.0400390625]|[0.9998311949763672,1.688050236328843E-4] |0         |[0.9998311949763672, 1.688050236328843E-4] |1.688050236328843E-4 |\n",
      "|-1.1185  |42.6563 |11582.4      |233.48  |0.0          |11612.88    |[-1.118499994277954,42.65629959106445,11582.400390625,233.47999572753906,0.0,11612.8798828125]                 |[0.999742398180602,2.576018193980714E-4]  |0         |[0.999742398180602, 2.576018193980714E-4]  |2.576018193980714E-4 |\n",
      "|-2.6232  |44.5791 |11582.4      |243.67  |-0.65        |11582.4     |[-2.623199939727783,44.5791015625,11582.400390625,243.6699981689453,-0.6499999761581421,11582.400390625]       |[0.9992816198544704,7.18380145529642E-4]  |0         |[0.9992816198544704, 7.18380145529642E-4]  |7.18380145529642E-4  |\n",
      "|7.4377   |44.1571 |11887.2      |219.16  |0.0          |11879.58    |[7.437699794769287,44.157100677490234,11887.2001953125,219.16000366210938,0.0,11879.580078125]                 |[0.9996402013420779,3.597986579222179E-4] |0         |[0.9996402013420779, 3.597986579222179E-4] |3.597986579222179E-4 |\n",
      "|9.199    |50.7192 |10538.46     |206.36  |-5.2         |10325.1     |[9.199000358581543,50.719200134277344,10538.4599609375,206.36000061035156,-5.199999809265137,10325.099609375]  |[0.90346474412926,0.09653525587074006]    |0         |[0.90346474412926, 0.09653525587074006]    |0.09653525587074006  |\n",
      "|-1.7548  |42.6552 |10668.0      |236.51  |-0.33        |10736.58    |[-1.7547999620437622,42.65520095825195,10668.0,236.50999450683594,-0.33000001311302185,10736.580078125]        |[0.9992592634901869,7.40736509813035E-4]  |0         |[0.9992592634901869, 7.40736509813035E-4]  |7.40736509813035E-4  |\n",
      "|6.5119   |46.4837 |13106.4      |210.54  |0.0          |13075.92    |[6.511899948120117,46.483699798583984,13106.400390625,210.5399932861328,0.0,13075.919921875]                   |[0.9993306563623914,6.6934363760863E-4]   |0         |[0.9993306563623914, 6.6934363760863E-4]   |6.6934363760863E-4   |\n",
      "|4.6614   |41.0592 |10668.0      |207.17  |0.0          |10721.34    |[4.661399841308594,41.059200286865234,10668.0,207.1699981689453,0.0,10721.33984375]                            |[0.9919922107572925,0.008007789242707522] |0         |[0.9919922107572925, 0.008007789242707522] |0.008007789242707522 |\n",
      "|8.7292   |44.8386 |10683.24     |216.78  |0.0          |10652.76    |[8.72920036315918,44.838600158691406,10683.240234375,216.77999877929688,0.0,10652.759765625]                   |[0.9974983567433072,0.0025016432566928788]|0         |[0.9974983567433072, 0.0025016432566928788]|0.0025016432566928788|\n",
      "+---------+--------+-------------+--------+-------------+------------+---------------------------------------------------------------------------------------------------------------+------------------------------------------+----------+-------------------------------------------+---------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4c2e675f66477ba1f958fd7d82e7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49ba9f800754090a7c4e005e8748e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+--------+-------------+------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+----------+-------------------------------------------+---------------------+\n",
      "|longitude|latitude|baro_altitude|velocity|vertical_rate|geo_altitude|features                                                                                                          |probability                               |prediction|probability_array                          |anomaly_score        |\n",
      "+---------+--------+-------------+--------+-------------+------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+----------+-------------------------------------------+---------------------+\n",
      "|8.4349   |47.6803 |3352.8       |166.42  |-5.53        |3329.94     |[8.434900283813477,47.680301666259766,3352.800048828125,166.4199981689453,-5.53000020980835,3329.93994140625]     |[0.9997734176603187,2.265823396813512E-4] |0         |[0.9997734176603187, 2.265823396813512E-4] |2.265823396813512E-4 |\n",
      "|6.3948   |50.1829 |9029.7       |203.81  |-9.1         |8869.68     |[6.394800186157227,50.182899475097656,9029.7001953125,203.80999755859375,-9.100000381469727,8869.6796875]         |[0.9991869224972976,8.13077502702445E-4]  |0         |[0.9991869224972976, 8.13077502702445E-4]  |8.13077502702445E-4  |\n",
      "|2.3008   |48.6729 |5631.18      |127.12  |9.43         |5570.22     |[2.300800085067749,48.67290115356445,5631.18017578125,127.12000274658203,9.430000305175781,5570.22021484375]      |[0.999249848703072,7.501512969280108E-4]  |0         |[0.999249848703072, 7.501512969280108E-4]  |7.501512969280108E-4 |\n",
      "|7.9869   |44.4528 |5974.08      |179.42  |-5.2         |6035.04     |[7.9868998527526855,44.45280075073242,5974.080078125,179.4199981689453,-5.199999809265137,6035.0400390625]        |[0.9996747778286186,3.252221713814768E-4] |0         |[0.9996747778286186, 3.252221713814768E-4] |3.252221713814768E-4 |\n",
      "|8.8093   |47.7328 |2979.42      |141.92  |-5.2         |2956.56     |[8.809300422668457,47.7327995300293,2979.419921875,141.9199981689453,-5.199999809265137,2956.56005859375]         |[0.999833628877761,1.663711222390432E-4]  |0         |[0.999833628877761, 1.663711222390432E-4]  |1.663711222390432E-4 |\n",
      "|-1.4981  |50.7247 |1501.14      |125.85  |-6.83        |1447.8      |[-1.4981000423431396,50.724700927734375,1501.1400146484375,125.8499984741211,-6.829999923706055,1447.800048828125]|[0.9987369018542753,0.0012630981457246318]|0         |[0.9987369018542753, 0.0012630981457246318]|0.0012630981457246318|\n",
      "|9.0782   |49.0221 |6888.48      |228.02  |-10.08       |6797.04     |[9.078200340270996,49.022098541259766,6888.47998046875,228.02000427246094,-10.079999923706055,6797.0400390625]    |[0.9990830413779946,9.169586220053362E-4] |0         |[0.9990830413779946, 9.169586220053362E-4] |9.169586220053362E-4 |\n",
      "|6.5125   |50.8248 |5676.9       |186.26  |11.7         |5516.88     |[6.512499809265137,50.824798583984375,5676.89990234375,186.25999450683594,11.699999809265137,5516.8798828125]     |[0.9990363502710646,9.636497289353493E-4] |0         |[0.9990363502710646, 9.636497289353493E-4] |9.636497289353493E-4 |\n",
      "|8.6044   |50.0444 |274.32       |72.92   |-3.9         |251.46      |[8.604399681091309,50.04439926147461,274.32000732421875,72.91999816894531,-3.9000000953674316,251.4600067138672]  |[0.9995037508929708,4.962491070292171E-4] |0         |[0.9995037508929708, 4.962491070292171E-4] |4.962491070292171E-4 |\n",
      "|9.4421   |44.2411 |8816.34      |210.62  |-4.23        |8862.06     |[9.442099571228027,44.2411003112793,8816.33984375,210.6199951171875,-4.230000019073486,8862.0595703125]           |[0.9984106503193241,0.0015893496806759942]|0         |[0.9984106503193241, 0.0015893496806759942]|0.0015893496806759942|\n",
      "|6.3194   |50.4028 |8282.94      |241.93  |-13.33       |8115.3      |[6.319399833679199,50.402801513671875,8282.9404296875,241.92999267578125,-13.329999923706055,8115.2998046875]     |[0.9986958230154344,0.001304176984565683] |0         |[0.9986958230154344, 0.001304176984565683] |0.001304176984565683 |\n",
      "|9.3056   |50.1371 |1325.88      |135.77  |0.33         |1257.3      |[9.3056001663208,50.13710021972656,1325.8800048828125,135.77000427246094,0.33000001311302185,1257.300048828125]   |[0.9995134757071397,4.865242928603173E-4] |0         |[0.9995134757071397, 4.865242928603173E-4] |4.865242928603173E-4 |\n",
      "|4.0452   |48.5555 |4975.86      |171.29  |-8.78        |4937.76     |[4.045199871063232,48.55550003051758,4975.85986328125,171.2899932861328,-8.779999732971191,4937.759765625]        |[0.999884636732369,1.153632676309052E-4]  |0         |[0.999884636732369, 1.153632676309052E-4]  |1.153632676309052E-4 |\n",
      "|9.5158   |50.6223 |3413.76      |129.51  |-4.88        |3307.08     |[9.515800476074219,50.62229919433594,3413.760009765625,129.50999450683594,-4.880000114440918,3307.080078125]      |[0.9996726778309697,3.273221690303057E-4] |0         |[0.9996726778309697, 3.273221690303057E-4] |3.273221690303057E-4 |\n",
      "|3.8485   |49.1654 |8366.76      |250.43  |6.18         |8267.7      |[3.8485000133514404,49.165401458740234,8366.759765625,250.42999267578125,6.179999828338623,8267.7001953125]       |[0.9993811992742971,6.188007257027692E-4] |0         |[0.9993811992742971, 6.188007257027692E-4] |6.188007257027692E-4 |\n",
      "|5.6525   |50.5814 |4137.66      |160.87  |-10.4        |4023.36     |[5.652500152587891,50.58140182495117,4137.66015625,160.8699951171875,-10.399999618530273,4023.360107421875]       |[0.9997641382444236,2.3586175557646852E-4]|0         |[0.9997641382444236, 2.3586175557646852E-4]|2.3586175557646852E-4|\n",
      "|8.7452   |50.0777 |868.68       |82.19   |-4.88        |838.2       |[8.745200157165527,50.077701568603516,868.6799926757812,82.19000244140625,-4.880000114440918,838.2000122070312]   |[0.9995441990959945,4.558009040055801E-4] |0         |[0.9995441990959945, 4.558009040055801E-4] |4.558009040055801E-4 |\n",
      "|9.0199   |45.4371 |1623.06      |115.34  |-8.78        |1653.54     |[9.01990032196045,45.43709945678711,1623.06005859375,115.33999633789062,-8.779999732971191,1653.5400390625]       |[0.9994418427521082,5.58157247891819E-4]  |0         |[0.9994418427521082, 5.58157247891819E-4]  |5.58157247891819E-4  |\n",
      "|8.6547   |47.4506 |868.68       |63.63   |-3.25        |868.68      |[8.65470027923584,47.450599670410156,868.6799926757812,63.630001068115234,-3.25,868.6799926757812]                |[0.9993615527601151,6.384472398848919E-4] |0         |[0.9993615527601151, 6.384472398848919E-4] |6.384472398848919E-4 |\n",
      "|-3.0021  |41.7823 |9913.62      |214.04  |4.88         |9974.58     |[-3.0020999908447266,41.78229904174805,9913.6201171875,214.0399932861328,4.880000114440918,9974.580078125]        |[0.9934234093658167,0.006576590634183303] |0         |[0.9934234093658167, 0.006576590634183303] |0.006576590634183303 |\n",
      "+---------+--------+-------------+--------+-------------+------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+----------+-------------------------------------------+---------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602c51e533a14078829ac34cf336523b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b66592658e74ea28ef9063019aaf0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+--------+-------------+------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------------+----------+-------------------------------------------+---------------------+\n",
      "|longitude|latitude|baro_altitude|velocity|vertical_rate|geo_altitude|features                                                                                                         |probability                               |prediction|probability_array                          |anomaly_score        |\n",
      "+---------+--------+-------------+--------+-------------+------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------------+----------+-------------------------------------------+---------------------+\n",
      "|-1.9601  |42.6757 |11887.2      |209.27  |0.0          |11940.54    |[-1.9601000547409058,42.67570114135742,11887.2001953125,209.27000427246094,0.0,11940.5400390625]                 |[0.9998595427208379,1.404572791621598E-4] |0         |[0.9998595427208379, 1.404572791621598E-4] |1.404572791621598E-4 |\n",
      "|2.8037   |41.7162 |11841.48     |213.16  |-2.6         |11910.06    |[2.8036999702453613,41.71620178222656,11841.48046875,213.16000366210938,-2.5999999046325684,11910.0595703125]    |[0.9888100765641072,0.011189923435892757] |0         |[0.9888100765641072, 0.011189923435892757] |0.011189923435892757 |\n",
      "|-1.1252  |46.8971 |11277.6      |205.54  |0.0          |11193.78    |[-1.1252000331878662,46.897098541259766,11277.599609375,205.5399932861328,0.0,11193.7802734375]                  |[0.9990969650456505,9.030349543495209E-4] |0         |[0.9990969650456505, 9.030349543495209E-4] |9.030349543495209E-4 |\n",
      "|0.2778   |41.0837 |11887.2      |207.21  |-0.33        |11971.02    |[0.2777999937534332,41.08369827270508,11887.2001953125,207.2100067138672,-0.33000001311302185,11971.01953125]    |[0.9998353335519815,1.646664480184833E-4] |0         |[0.9998353335519815, 1.646664480184833E-4] |1.646664480184833E-4 |\n",
      "|-1.8454  |50.2608 |11292.84     |217.88  |-0.33        |11041.38    |[-1.8453999757766724,50.260799407958984,11292.83984375,217.8800048828125,-0.33000001311302185,11041.3798828125]  |[0.9998795504510868,1.2044954891310006E-4]|0         |[0.9998795504510868, 1.2044954891310006E-4]|1.2044954891310006E-4|\n",
      "|-1.5765  |47.3055 |11277.6      |203.45  |0.0          |11163.3     |[-1.5765000581741333,47.30550003051758,11277.599609375,203.4499969482422,0.0,11163.2998046875]                   |[0.9995276975019485,4.723024980514752E-4] |0         |[0.9995276975019485, 4.723024980514752E-4] |4.723024980514752E-4 |\n",
      "|4.4479   |42.9509 |11894.82     |242.82  |0.0          |11917.68    |[4.44789981842041,42.95090103149414,11894.8203125,242.82000732421875,0.0,11917.6796875]                          |[0.9984077473045959,0.0015922526954040774]|0         |[0.9984077473045959, 0.0015922526954040774]|0.0015922526954040774|\n",
      "|-2.3816  |45.0517 |11871.96     |212.11  |0.33         |11856.72    |[-2.3815999031066895,45.051700592041016,11871.9599609375,212.11000061035156,0.33000001311302185,11856.7197265625]|[0.999497956582867,5.020434171329424E-4]  |0         |[0.999497956582867, 5.020434171329424E-4]  |5.020434171329424E-4 |\n",
      "|-4.5739  |41.2962 |11277.6      |222.87  |-0.33        |11346.18    |[-4.57390022277832,41.296199798583984,11277.599609375,222.8699951171875,-0.33000001311302185,11346.1796875]      |[0.9998320883057541,1.679116942459505E-4] |0         |[0.9998320883057541, 1.679116942459505E-4] |1.679116942459505E-4 |\n",
      "|8.5283   |48.8786 |10980.42     |211.25  |-0.33        |10835.64    |[8.528300285339355,48.87860107421875,10980.419921875,211.25,-0.33000001311302185,10835.6396484375]               |[0.9977728256709751,0.0022271743290248006]|0         |[0.9977728256709751, 0.0022271743290248006]|0.0022271743290248006|\n",
      "|5.5534   |50.875  |11582.4      |203.7   |0.33         |11376.66    |[5.553400039672852,50.875,11582.400390625,203.6999969482422,0.33000001311302185,11376.66015625]                  |[0.999014825286894,9.851747131060299E-4]  |0         |[0.999014825286894, 9.851747131060299E-4]  |9.851747131060299E-4 |\n",
      "|-0.7564  |42.7123 |11582.4      |233.87  |0.0          |11612.88    |[-0.7563999891281128,42.71229934692383,11582.400390625,233.8699951171875,0.0,11612.8798828125]                   |[0.9994501996223781,5.498003776219571E-4] |0         |[0.9994501996223781, 5.498003776219571E-4] |5.498003776219571E-4 |\n",
      "|-2.561   |44.8602 |11582.4      |243.67  |0.33         |11567.16    |[-2.561000108718872,44.860198974609375,11582.400390625,243.6699981689453,0.33000001311302185,11567.16015625]     |[0.9987752659112462,0.0012247340887538898]|0         |[0.9987752659112462, 0.0012247340887538898]|0.0012247340887538898|\n",
      "|7.1301   |44.0263 |11887.2      |221.27  |0.0          |11887.2     |[7.130099773406982,44.02629852294922,11887.2001953125,221.27000427246094,0.0,11887.2001953125]                   |[0.9993227547502173,6.772452497828271E-4] |0         |[0.9993227547502173, 6.772452497828271E-4] |6.772452497828271E-4 |\n",
      "|9.1513   |50.4819 |10363.2      |203.87  |0.0          |10149.84    |[9.151300430297852,50.48189926147461,10363.2001953125,203.8699951171875,0.0,10149.83984375]                      |[0.998740529811762,0.00125947018823812]   |0         |[0.998740529811762, 0.00125947018823812]   |0.00125947018823812  |\n",
      "|-1.921   |42.4067 |10668.0      |239.27  |0.0          |10744.2     |[-1.9210000038146973,42.406700134277344,10668.0,239.27000427246094,0.0,10744.2001953125]                         |[0.9988345582388459,0.0011654417611541967]|0         |[0.9988345582388459, 0.0011654417611541967]|0.0011654417611541967|\n",
      "|4.3438   |41.0924 |10660.38     |208.37  |0.33         |10713.72    |[4.343800067901611,41.09239959716797,10660.3798828125,208.3699951171875,0.33000001311302185,10713.7197265625]    |[0.9899559813348814,0.01004401866511864]  |0         |[0.9899559813348814, 0.01004401866511864]  |0.01004401866511864  |\n",
      "|9.4173   |48.3739 |11269.98     |247.03  |0.33         |11140.44    |[9.4173002243042,48.3739013671875,11269.98046875,247.02999877929688,0.33000001311302185,11140.4404296875]        |[0.9915032888060892,0.008496711193910801] |0         |[0.9915032888060892, 0.008496711193910801] |0.008496711193910801 |\n",
      "|8.4571   |44.6779 |10660.38     |214.4   |-0.33        |10637.52    |[8.457099914550781,44.67789840698242,10660.3798828125,214.39999389648438,-0.33000001311302185,10637.51953125]    |[0.9970900040192462,0.0029099959807537628]|0         |[0.9970900040192462, 0.0029099959807537628]|0.0029099959807537628|\n",
      "|7.4322   |50.2964 |10668.0      |249.2   |0.0          |10454.64    |[7.432199954986572,50.2963981628418,10668.0,249.1999969482422,0.0,10454.6396484375]                              |[0.9931939284046154,0.006806071595384623] |0         |[0.9931939284046154, 0.006806071595384623] |0.006806071595384623 |\n",
      "+---------+--------+-------------+--------+-------------+------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------------+----------+-------------------------------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "Stream stopped after 3 minutes.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, LongType\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.functions import vector_to_array  # Import this for vector conversion\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightAnomalyDetection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Event Hubs connection string\n",
    "connection_str = \"Endpoint=sb://flightproject.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=##;EntityPath=flightStream2\"\n",
    "encrypted_connection_str = spark.sparkContext._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_str)\n",
    "\n",
    "# Event Hubs configuration\n",
    "ehConf = {\n",
    "    'eventhubs.connectionString': encrypted_connection_str,\n",
    "    'eventhubs.eventHubName': 'flightStream2',\n",
    "}\n",
    "\n",
    "# Define schema for incoming JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"icao24\", StringType(), True),\n",
    "    StructField(\"callsign\", StringType(), True),\n",
    "    StructField(\"origin_country\", StringType(), True),\n",
    "    StructField(\"time_position\", LongType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"baro_altitude\", FloatType(), True),\n",
    "    StructField(\"velocity\", FloatType(), True),\n",
    "    StructField(\"vertical_rate\", FloatType(), True),\n",
    "    StructField(\"geo_altitude\", FloatType(), True),\n",
    "    StructField(\"on_ground\", BooleanType(), True),\n",
    "])\n",
    "\n",
    "# Read streaming data from Event Hubs\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**ehConf) \\\n",
    "    .load()\n",
    "\n",
    "# Decode and parse the JSON data\n",
    "parsed_stream = raw_stream.select(from_json(col(\"body\").cast(\"string\"), schema).alias(\"data\"))\n",
    "final_stream = parsed_stream.select(\n",
    "    \"data.icao24\", \"data.callsign\", \"data.origin_country\", \"data.time_position\",\n",
    "    \"data.longitude\", \"data.latitude\", \"data.baro_altitude\", \"data.velocity\", \n",
    "    \"data.vertical_rate\", \"data.geo_altitude\", \"data.on_ground\"\n",
    ").na.drop()\n",
    "\n",
    "# Function to prepare features for GMM\n",
    "def prepare_features(df):\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"longitude\", \"latitude\", \"baro_altitude\", \"velocity\", \"vertical_rate\", \"geo_altitude\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    return assembler.transform(df)\n",
    "\n",
    "# Function to train GMM and detect anomalies\n",
    "def train_gmm(batch_df, epoch_id):\n",
    "    if batch_df.count() > 0:  # Train only if there is data\n",
    "        # Prepare features for GMM\n",
    "        feature_df = prepare_features(batch_df)\n",
    "        \n",
    "        # Train Gaussian Mixture Model\n",
    "        gmm = GaussianMixture(featuresCol=\"features\", k=2, maxIter=10)\n",
    "        model = gmm.fit(feature_df)\n",
    "        \n",
    "        # Predict cluster for each point and calculate probabilities\n",
    "        predictions = model.transform(feature_df)\n",
    "\n",
    "        # Convert `probability` vector to an array and extract the second element (anomaly score)\n",
    "        predictions = predictions.withColumn(\"probability_array\", vector_to_array(col(\"probability\")))\n",
    "        predictions = predictions.withColumn(\"anomaly_score\", col(\"probability_array\")[1])\n",
    "\n",
    "        # Detect anomalies based on threshold\n",
    "        threshold = 0.1\n",
    "        anomalies = predictions.filter(col(\"anomaly_score\") < threshold)\n",
    "        anomalies.show(truncate=False)\n",
    "\n",
    "# Apply transformations to the streaming data\n",
    "feature_stream = final_stream.select(\n",
    "    col(\"longitude\"), col(\"latitude\"), col(\"baro_altitude\"), col(\"velocity\"), col(\"vertical_rate\"), col(\"geo_altitude\")\n",
    ")\n",
    "\n",
    "# Write stream with a timer\n",
    "start_time = time.time()  # Get the start time\n",
    "\n",
    "query = feature_stream.writeStream.foreachBatch(train_gmm).start()\n",
    "\n",
    "# Run the stream for only 3 minutes\n",
    "while time.time() - start_time < 3 * 60:  # Run for 180 seconds\n",
    "    time.sleep(1)  # Check every second\n",
    "\n",
    "# Stop the stream after 3 minutes\n",
    "query.stop()\n",
    "print(\"Stream stopped after 3 minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "801e8e1a-9264-41af-8338-b2e1436ee9b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, udf, unix_timestamp, when, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, LongType, IntegerType\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightAnomalyDetection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Event Hubs connection string\n",
    "connection_str = \"Endpoint=sb://flightproject.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=##;EntityPath=flightStream2\"\n",
    "encrypted_connection_str = spark.sparkContext._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_str)\n",
    "\n",
    "# Event Hubs configuration\n",
    "ehConf = {\n",
    "    'eventhubs.connectionString': encrypted_connection_str,\n",
    "    'eventhubs.eventHubName': 'flightStream2',\n",
    "}\n",
    "\n",
    "# Define schema for incoming JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"icao24\", StringType(), True),\n",
    "    StructField(\"callsign\", StringType(), True),\n",
    "    StructField(\"origin_country\", StringType(), True),\n",
    "    StructField(\"time_position\", LongType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"baro_altitude\", FloatType(), True),\n",
    "    StructField(\"velocity\", FloatType(), True),\n",
    "    StructField(\"vertical_rate\", FloatType(), True),\n",
    "    StructField(\"geo_altitude\", FloatType(), True),\n",
    "    StructField(\"on_ground\", BooleanType(), True),\n",
    "])\n",
    "\n",
    "# Read streaming data from Event Hubs\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**ehConf) \\\n",
    "    .load()\n",
    "\n",
    "# Decode and parse the JSON data\n",
    "parsed_stream = raw_stream.select(from_json(col(\"body\").cast(\"string\"), schema).alias(\"data\"))\n",
    "processed_stream = parsed_stream.select(\n",
    "    col(\"data.icao24\").alias(\"icao24\"),\n",
    "    col(\"data.callsign\").alias(\"callsign\"),\n",
    "    col(\"data.time_position\").alias(\"time\").cast(\"int\"),  # Rename and cast type\n",
    "    col(\"data.latitude\").alias(\"lat\").cast(\"double\"),\n",
    "    col(\"data.longitude\").alias(\"lon\").cast(\"double\"),\n",
    "    col(\"data.velocity\").alias(\"velocity\").cast(\"double\"),\n",
    "    col(\"data.vertical_rate\").alias(\"vertrate\").cast(\"double\"),\n",
    "    col(\"data.baro_altitude\").alias(\"baroaltitude\").cast(\"double\"),\n",
    "    col(\"data.geo_altitude\").alias(\"geoaltitude\").cast(\"double\"),\n",
    "    col(\"data.on_ground\").alias(\"onground\").cast(\"boolean\")\n",
    ").na.drop()  # Drop rows with null values in selected columns\n",
    "\n",
    "# Add missing columns with default values\n",
    "processed_stream = processed_stream.withColumn(\"alert\", lit(None).cast(\"boolean\")) \\\n",
    "                                   .withColumn(\"spi\", lit(None).cast(\"boolean\")) \\\n",
    "                                   .withColumn(\"squawk\", lit(None).cast(\"int\")) \\\n",
    "                                   .withColumn(\"lastposupdate\", lit(None).cast(\"double\")) \\\n",
    "                                   .withColumn(\"lastcontact\", lit(None).cast(\"double\"))\n",
    "start_time = time.time()\n",
    "\n",
    "# Write the transformed stream to Cassandra\n",
    "query= processed_stream.writeStream \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"flightdata\", keyspace=\"flightdb\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"spark.cassandra.connection.ssl.enabled\", \"true\") \\\n",
    "    .option(\"spark.cassandra.auth.username\", \"cassandraaccount\") \\\n",
    "    .option(\"spark.cassandra.auth.password\", \"##\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/flightdata\") \\\n",
    "    .start()\n",
    "\n",
    "while time.time() - start_time < 2 * 60:  # Run for 120 seconds\n",
    "    time.sleep(1)  # Check every second\n",
    "\n",
    "# Stop the stream after 2 minutes\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "805fcde1-fb09-4bba-93e8-875e15b71db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.cassandra.connection.host\", \"cassandraaccount.cassandra.cosmos.azure.com\")\n",
    "spark.conf.set(\"spark.cassandra.connection.port\", \"10350\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FlightAnomaliesDetection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
